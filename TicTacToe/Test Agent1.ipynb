{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BaseAgent import BaseAgent\n",
    "from BaseEnvironment import BaseEnvironment\n",
    "from RLGlue import RLGlue\n",
    "from Softmax import softmax\n",
    "from Adam import Adam\n",
    "from SimpleNN import SimpleNN\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeEvnironment(BaseEnvironment):\n",
    "    def env_init(self, env_info={}):\n",
    "        pass\n",
    "    \n",
    "    def env_start(self):\n",
    "        self.terminal = False\n",
    "        self.board = np.zeros((3, 3))\n",
    "        self.reward_obs_term = (0, self.board, False)\n",
    "        return self.board.reshape(1,-1), self.get_mask()\n",
    "    \n",
    "    def env_step(self, agent_num, index):\n",
    "        if self.terminal:\n",
    "            print(\"Environment in terminal state, please restart.\")\n",
    "        \n",
    "        row, col = self.transform_index(index)\n",
    "        self.board[row, col] = agent_num\n",
    "        \n",
    "        if self.check_won(agent_num):\n",
    "            reward = 10\n",
    "            self.terminal = True\n",
    "        elif self.check_tie():\n",
    "            reward = 0\n",
    "            self.terminal = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        self.reward_obs_term_mask = (reward, self.board.reshape(1,-1), self.terminal, self.get_mask())\n",
    "        return self.reward_obs_term_mask\n",
    "    \n",
    "    def check_tie(self):\n",
    "        return (self.board == 0).sum() == 0\n",
    "    \n",
    "    def check_won(self, agent_num):\n",
    "        for row in self.board:\n",
    "            if np.array_equal(row, agent_num * np.ones((3,))):\n",
    "                return True\n",
    "        for col in self.board.T:\n",
    "            if np.array_equal(col, agent_num * np.ones((3,))):\n",
    "                return True\n",
    "        diag = np.diag(self.board)\n",
    "        if np.array_equal(diag, agent_num * np.ones((3,))):\n",
    "            return True\n",
    "        diag = np.diag(np.fliplr(self.board))\n",
    "        if np.array_equal(diag, agent_num * np.ones((3,))):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def env_cleanup(self):\n",
    "        pass\n",
    "    \n",
    "    def env_message(self, message):\n",
    "        if message == 0:  # return available indices mask\n",
    "            return self.get_mask()\n",
    "            \n",
    "    def get_mask(self):\n",
    "        rows, cols = np.where(self.board == 0)\n",
    "        indices = rows * 3 + cols\n",
    "        mask = np.zeros((9,))\n",
    "        mask[indices] = 1\n",
    "        return mask\n",
    "    \n",
    "    def transform_index(self, index):\n",
    "        return index // 3, index % 3\n",
    "    \n",
    "    \n",
    "class TicTacToeAgent(BaseAgent):\n",
    "    def agent_init(self, agent_init_info):\n",
    "        self.discount = agent_init_info[\"discount\"]        \n",
    "        self.network = agent_init_info[\"network\"]\n",
    "        self.optimizer = agent_init_info[\"optimizer\"]\n",
    "        self.tau = agent_init_info[\"tau\"]\n",
    "        self.num_actions = agent_init_info[\"num_actions\"]\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_init_info[\"seed\"])\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def policy(self, state, mask):\n",
    "        action_values = self.network.get_action_values(state)\n",
    "        probs = softmax(action_values, self.tau) \n",
    "        probs *= mask\n",
    "        probs /= probs.sum()\n",
    "        action = self.rand_generator.choice(self.num_actions, p=probs.squeeze())\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state, mask):\n",
    "        self.last_state = state\n",
    "        self.last_action = self.policy(self.last_state, mask)\n",
    "        return self.last_action        \n",
    "\n",
    "    def agent_step(self, reward, state, mask):\n",
    "        # SARSA\n",
    "        action = self.policy(state, mask)\n",
    "        self.network.get_action_values(state)\n",
    "        delta = reward + self.discount * self.network.get_action_values(state)[action] - \\\n",
    "                    self.network.get_action_values(self.last_state)[self.last_action]        \n",
    "        delta_mat = np.zeros((1,self.num_actions))\n",
    "        delta_mat[0, self.last_action] = delta\n",
    "        \n",
    "        grads = self.network.get_gradients(self.last_state, delta_mat)\n",
    "        self.optimizer.update_weights(self.network.get_weights(), grads)\n",
    "        \n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        # SARSA\n",
    "        delta = reward - self.network.get_action_values(self.last_state)[self.last_action]        \n",
    "        delta_mat = np.zeros((1,self.num_actions))\n",
    "        delta_mat[0, self.last_action] = delta\n",
    "        \n",
    "        grads = self.network.get_gradients(self.last_state, delta_mat)\n",
    "        self.optimizer.update_weights(self.network.get_weights(), grads)\n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = open(\"agent-1-network-weights\",'rb')\n",
    "weights = pickle.load(fr)\n",
    "fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers1 = [9, 6, 9]\n",
    "nn = SimpleNN({\"layer_sizes\": layers1, \"seed\": 11})\n",
    "nn.set_weights(weights)\n",
    "optimizer_info = { \"step_size\": .1,\n",
    "                  \"beta_m\": 0.99,\n",
    "                  \"beta_v\": 0.999,\n",
    "                  \"epsilon\": 0.0001 }\n",
    "optimizer = Adam(layers1, optimizer_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce tau to choose more optimal actions\n",
    "agent_info = {\"discount\": 1, \"network\": nn, \"optimizer\": optimizer, \"tau\": 0.1, \"num_actions\": 9, \"seed\": 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = TicTacToeAgent()\n",
    "ai.agent_init(agent_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToeEvnironment()\n",
    "env.env_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, mask = env.env_start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human\n",
    "reward, state, ter, mask = env.env_step(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ai.agent_start(state, mask)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai\n",
    "reward, state, ter, mask = env.env_step(-1, action)\n",
    "# human\n",
    "reward, state, ter, mask = env.env_step(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.47968785, -6.5683146 , -3.30616794,  0.10791578,  0.51350736,\n",
       "        0.38507136, -1.97239166, -5.88189067, -4.42064227])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai.network.get_action_values(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ai.agent_start(state, mask)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai\n",
    "reward, state, ter, mask = env.env_step(-1, action)\n",
    "# human\n",
    "reward, state, ter, mask = env.env_step(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ai.agent_start(state, mask)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai\n",
    "reward, state, ter, mask = env.env_step(-1, action)\n",
    "# human\n",
    "reward, state, ter, mask = env.env_step(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = ai.agent_start(state, mask)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai\n",
    "reward, state, ter, mask = env.env_step(-1, action)\n",
    "# human\n",
    "reward, state, ter, mask = env.env_step(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human won!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai.agent_end(-reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
